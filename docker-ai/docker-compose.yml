models:
  qwen3:
    model: ai/qwen3:0.6B-Q4_0

services:

  #
  # https://github.com/docker/mcp-gateway/blob/main/examples/container/README.md
  # https://github.com/docker/mcp-gateway/blob/main/examples/interceptors/compose.yaml
  #
  mcp-gateway:
    image: docker/mcp-gateway
    command:
      - --transport=streaming
      - --servers=curl
      - --servers=github
      - --verbose=true
      - --port=9011
    ports:
      - 9011:9011
    volumes:
      - //var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: wget -O- http://localhost:9011/health
      interval: 1s
      timeout: 2s
      retries: 60
      start_period: 2s

  chat-backend:
    build: .
    environment:
      - MCP_HOST=http://mcp-gateway:9011/mcp
    ports:
      - "3001:3001"
    models:
      qwen3:
        endpoint_var: MODEL_BASE_URL
    depends_on:
      mcp-gateway:
        condition: service_healthy

  #
  # docker exec -it ollama ollama pull phi3:mini-4k
  #
  # https://ollama.com/library/qwen3
  # docker exec -it ollama ollama pull qwen3:0.6b
  #
  ollama:
    container_name: ollama
    image: ollama/ollama
    pull_policy: always
    healthcheck:
      test: ollama ps || exit 1
      interval: 10s
    ports:
      - "11435:11434"
    # Persistent storage for models and configuration
    #    volumes:
    #      - ./ollama:/root/.ollama
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_HOST=0.0.0.0
      # Keep models loaded in memory for 5 minutes after last use
      - OLLAMA_KEEP_ALIVE=5m
      # Flash attention (0=disabled, 1=enabled but experimental)
      - OLLAMA_FLASH_ATTENTION=0
    # GPU Configuration (uncomment for GPU acceleration)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
